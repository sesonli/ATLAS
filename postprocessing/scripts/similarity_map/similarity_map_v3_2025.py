import sqlite3
import json
import itertools
from collections import defaultdict
import numpy as np
from scipy.optimize import linear_sum_assignment
import networkx as nx
import time
from grakel import Graph
from grakel.kernels import WeisfeilerLehman, EdgeHistogram, VertexHistogram

def compute_motif_similarity_4(motif1, motif2):
    """
    Compute S_mn using Weisfeiler-Lehman Graph Kernel.
    """
    G1 = build_motif_graph(motif1)
    G2 = build_motif_graph(motif2)
    
    # Convert to Grakel format
    g1 = Graph(nx.adjacency_matrix(G1).todense().tolist())
    g2 = Graph(nx.adjacency_matrix(G2).todense().tolist())

    # Compute WL kernel similarity
    wl_kernel = WeisfeilerLehman(n_iter=3, base_kernel=VertexHistogram)
    similarity_matrix = wl_kernel.fit_transform([g1, g2])
    
    return similarity_matrix[0, 1]  # Similarity score in [0,1]


# -------------------------
# Database Loading
# -------------------------
def load_data(db_path):
    """Load RNA motif data from SQLite database"""
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("SELECT * FROM data")
    rows = cursor.fetchall()
    columns = [desc[0] for desc in cursor.description]
    data = [dict(zip(columns, row)) for row in rows]
    conn.close()
    return data

# -------------------------
# Group Motifs by RNA
# -------------------------
def group_motifs_by_RNA(data):
    """Group all motifs (all types) by RNA (using pdbid as key)"""
    RNA_motifs = defaultdict(list)
    for motif in data:
        RNA_motifs[motif['pdbid'].lower()].append(motif)
    return RNA_motifs

# -------------------------
# Motif Weight Calculation
# -------------------------
def compute_motif_weight(motif):
    """Compute weight as: w(m) = (# nucleotides) * (1 + # noncanonical bp)"""
    if not motif.get('nt_number'):
        return 0
    nucleotides = motif['nt_number'].split(',')
    L = len(nucleotides)
    NC = 0
    try:
        edges = json.loads(motif['non_graph_edges'])
    except Exception:
        edges = []
    for edge in edges:
        if isinstance(edge[2], dict) and edge[2].get("attribute") == [0, 1, 0]:
            NC += 1
    return L * (1 + NC)

# -------------------------
# Build Motif Graph
# -------------------------
def build_motif_graph(motif):
    """Build a graph from the motif's non_graph_edges (all bond types)"""
    G = nx.Graph()
    if not motif.get('nt_number'):
        return G
    nucleotides = [nt.strip() for nt in motif['nt_number'].split(',')]
    G.add_nodes_from(nucleotides)
    try:
        edges = json.loads(motif['non_graph_edges'])
        for edge in edges:
            node1 = str(edge[0]).strip()
            node2 = str(edge[1]).strip()
            # Store edge attributes
            attr = edge[2].get("attribute", []) if isinstance(edge[2], dict) else []
            if node1 in G.nodes and node2 in G.nodes:
                G.add_edge(node1, node2, attr=attr)
    except Exception:
        pass
    return G

# -------------------------
# Motif Similarity Functions
# -------------------------
def compute_motif_similarity_1(motif1, motif2):
    """
    Compute S_mn = 1 - (D_mn / D_max) using graph_edit_distance.
    D_max = (N1 + N2) + (E1 + E2) with unit costs.
    """
    G1 = build_motif_graph(motif1)
    G2 = build_motif_graph(motif2)
    N1, E1 = G1.number_of_nodes(), G1.number_of_edges()
    N2, E2 = G2.number_of_nodes(), G2.number_of_edges()
    D_max = (N1 + N2) + (E1 + E2)
    try:
        D_mn = nx.graph_edit_distance(G1, G2, timeout=1)
    except Exception:
        D_mn = D_max / 2.0
    if D_mn is None:
        D_mn = D_max / 2.0
    return 1 - (D_mn / D_max) if D_max > 0 else 0.0 

def compute_motif_similarity_2(motif1, motif2):
    """Heuristic: Return 1 if non_graph_edges strings match exactly, else 0."""
    return 1.0 if motif1['non_graph_edges'] == motif2['non_graph_edges'] else 0.0

def compute_motif_similarity_3(motif1, motif2):
    """
    Lightweight heuristic comparing basic features:
    S = 1 - (|L1 - L2| + |E1 - E2|) / (L1 + L2 + E1 + E2)
    """
    L1 = len(motif1['nt_number'].split(',')) if motif1.get('nt_number') else 0
    L2 = len(motif2['nt_number'].split(',')) if motif2.get('nt_number') else 0
    try:
        E1 = len(json.loads(motif1['non_graph_edges']))
    except:
        E1 = 0
    try:
        E2 = len(json.loads(motif2['non_graph_edges']))
    except:
        E2 = 0
    numerator = abs(L1 - L2) + abs(E1 - E2)
    denominator = (L1 + L2 + E1 + E2)
    return 1 - (numerator / denominator) if denominator > 0 else 0.0

# -------------------------
# New Hybrid Graph Similarity Function
# -------------------------
def compute_hybrid_graph_similarity(motif1, motif2):
    """
    Compute graph similarity using a hybrid approach that considers:
    1. Structural similarity (based on graph properties)
    2. Edge attribute similarity
    """
    G1 = build_motif_graph(motif1)
    G2 = build_motif_graph(motif2)
    
    # If either graph is empty, return appropriate similarity
    if G1.number_of_nodes() == 0 and G2.number_of_nodes() == 0:
        return 1.0
    if G1.number_of_nodes() == 0 or G2.number_of_nodes() == 0:
        return 0.0
    
    # Check for trivial cases first
    if e1 == 0 and e2 == 0:
        return 1.0  # Both have no edges, so they are identical (trivial case)
    elif e1 == 0 or e2 == 0:
        return 0.0  # One has edges, one doesn't - they're fundamentally different
    
    # For RNA motifs, there are two key aspects to consider:
    # 1. The structural topology (connections between nucleotides)
    # 2. The base-pairing types (edge attributes)
    
    # Both graphs have same number of nodes, check for structural equivalence
    if nx.is_isomorphic(G1, G2):
        # Perfect structural match
        structure_sim = 1.0
    else:
        # No perfect structural match, compute approximate similarity using Jaccard similarity
        # of node-pair sets (which nucleotides are connected)
        edge_set1 = {frozenset((str(e[0]), str(e[1]))) for e in G1.edges()}
        edge_set2 = {frozenset((str(e[0]), str(e[1]))) for e in G2.edges()}
        
        # Jaccard similarity: |intersection| / |union|
        intersection = len(edge_set1.intersection(edge_set2))
        union = len(edge_set1.union(edge_set2))
        structure_sim = intersection / union if union > 0 else 0.0
    
    # For edge attributes, we are specifically interested in RNA bond types
    # Count edge attributes in each graph
    attr_count1 = {}
    attr_count2 = {}
    
    for _, _, attr in G1.edges(data=True):
        attr_tuple = tuple(attr.get('attr', []))
        attr_count1[attr_tuple] = attr_count1.get(attr_tuple, 0) + 1
    
    for _, _, attr in G2.edges(data=True):
        attr_tuple = tuple(attr.get('attr', []))
        attr_count2[attr_tuple] = attr_count2.get(attr_tuple, 0) + 1
    
    # Compute Cosine similarity between attribute count vectors
    # This measure is invariant to the absolute counts and focuses on the proportions
    all_attrs = set(attr_count1.keys()).union(attr_count2.keys())
    
    # Build vectors from the attribute counts
    vec1 = [attr_count1.get(attr, 0) for attr in all_attrs]
    vec2 = [attr_count2.get(attr, 0) for attr in all_attrs]
    
    # Compute cosine similarity: dot(vec1, vec2) / (||vec1|| * ||vec2||)
    dot_product = sum(a * b for a, b in zip(vec1, vec2))
    norm1 = sum(a * a for a in vec1) ** 0.5
    norm2 = sum(b * b for b in vec2) ** 0.5
    attr_sim = dot_product / (norm1 * norm2) if norm1 > 0 and norm2 > 0 else 0.0
    
    # Final similarity is a geometric mean of structure and attribute similarity
    # This represents that both aspects are equally important and multiplicative
    # If either is 0, the result is 0 (can't have equivalent RNA motifs with completely
    # different structures or completely different bond types)
    return (structure_sim * attr_sim) ** 0.5

# -------------------------
# Graph Similarity Function using Adjacency Matrix
# -------------------------
def compute_spectral_similarity(motif1, motif2):
    """
    Compute graph similarity using eigendecomposition of adjacency matrices.
    This is a well-established method that captures structural similarity.
    
    The similarity is based on comparing the set of eigenvalues of the adjacency matrices,
    which characterize the graph structure.
    """
    G1 = build_motif_graph(motif1)
    G2 = build_motif_graph(motif2)
    
    # Handle trivial cases
    if G1.number_of_nodes() == 0 and G2.number_of_nodes() == 0:
        return 1.0
    if G1.number_of_nodes() == 0 or G2.number_of_nodes() == 0:
        return 0.0
    
    # Get adjacency matrices
    A1 = nx.adjacency_matrix(G1).todense()
    A2 = nx.adjacency_matrix(G2).todense()
    
    # Check for exact graph isomorphism first (fastest path for identical graphs)
    # This handles the case where the graphs are structurally identical
    if A1.shape == A2.shape and np.all(A1 == A2):
        return 1.0
    
    # Now compute eigenvalues
    try:
        eig1 = np.linalg.eigvals(A1)
        eig2 = np.linalg.eigvals(A2)
        
        # Sort the eigenvalues
        eig1 = sorted(abs(eig1))
        eig2 = sorted(abs(eig2))
        
        # Pad the smaller set with zeros to make lengths equal
        if len(eig1) < len(eig2):
            eig1 = eig1 + [0] * (len(eig2) - len(eig1))
        elif len(eig2) < len(eig1):
            eig2 = eig2 + [0] * (len(eig1) - len(eig2))
        
        # Calculate similarity based on eigenvalue difference
        # This is a standard approach in spectral graph theory
        diff_sum = sum((e1 - e2)**2 for e1, e2 in zip(eig1, eig2))
        max_sum = sum(max(e1**2, e2**2) for e1, e2 in zip(eig1, eig2))
        
        if max_sum == 0:
            return 1.0
            
        similarity = 1 - (diff_sum / max_sum)**0.5
        return max(0, min(1, similarity))  # Ensure in range [0,1]
        
    except Exception as e:
        # Fallback to a simpler size-based similarity metric
        n1, e1 = G1.number_of_nodes(), G1.number_of_edges()
        n2, e2 = G2.number_of_nodes(), G2.number_of_edges()
        
        if n1 + n2 + e1 + e2 == 0:
            return 1.0
            
        return 1 - (abs(n1-n2) + abs(e1-e2)) / (n1 + n2 + e1 + e2)

def enhanced_wl_kernel_similarity(motif1, motif2):
    """
    Use GraKeL's graph kernels to compare graphs with edge attributes.
    This is an established algorithm that properly handles both structure and edge attributes.
    """
    G1 = build_motif_graph(motif1)
    G2 = build_motif_graph(motif2)
    
    # Handle trivial cases
    if G1.number_of_nodes() == 0 and G2.number_of_nodes() == 0:
        return 1.0
    if G1.number_of_nodes() == 0 or G2.number_of_nodes() == 0:
        return 0.0
    
    try:
        # åˆ›å»ºåŸºäºè¾¹å±æ€§çš„æ ¸å‡½æ•°
        base_kernel = VertexHistogram(normalize=True)
        wl_kernel = WeisfeilerLehman(n_iter=3, base_kernel=base_kernel, normalize=True)
        
        # ä¸ºèŠ‚ç‚¹åˆ›å»ºæ ‡ç­¾ï¼ˆä½¿ç”¨åº¦æ•°ï¼‰
        node_labels1 = {node: str(G1.degree(node)) for node in G1.nodes()}
        node_labels2 = {node: str(G2.degree(node)) for node in G2.nodes()}
        
        # ä¸ºè¾¹åˆ›å»ºæ ‡ç­¾ï¼ˆå°†RNAé”®ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ‡ç­¾ï¼‰
        edge_labels1 = {}
        for u, v, attr in G1.edges(data=True):
            edge_key = (u, v) if u <= v else (v, u)  # ç¡®ä¿ä¸€è‡´çš„é”®é¡ºåº
            attr_list = attr.get('attr', [])
            
            # å°†RNAé”®ç±»å‹æ•°ç»„è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ‡ç­¾
            if len(attr_list) >= 3:
                # [0,0,1] = covalent, [1,0,0] = watson_crick, [0,1,0] = non_watson_crick
                if attr_list == [0, 0, 1]:
                    edge_label = "covalent"
                elif attr_list == [1, 0, 0]:
                    edge_label = "watson_crick"
                elif attr_list == [0, 1, 0]:
                    edge_label = "non_watson_crick"
                else:
                    edge_label = f"custom_{attr_list[0]}_{attr_list[1]}_{attr_list[2]}"
            else:
                edge_label = "unknown"
            
            edge_labels1[edge_key] = edge_label
        
        edge_labels2 = {}
        for u, v, attr in G2.edges(data=True):
            edge_key = (u, v) if u <= v else (v, u)  # ç¡®ä¿ä¸€è‡´çš„é”®é¡ºåº
            attr_list = attr.get('attr', [])
            
            # å°†RNAé”®ç±»å‹æ•°ç»„è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ‡ç­¾
            if len(attr_list) >= 3:
                # [0,0,1] = covalent, [1,0,0] = watson_crick, [0,1,0] = non_watson_crick
                if attr_list == [0, 0, 1]:
                    edge_label = "covalent"
                elif attr_list == [1, 0, 0]:
                    edge_label = "watson_crick"
                elif attr_list == [0, 1, 0]:
                    edge_label = "non_watson_crick"
                else:
                    edge_label = f"custom_{attr_list[0]}_{attr_list[1]}_{attr_list[2]}"
            else:
                edge_label = "unknown"
            
            edge_labels2[edge_key] = edge_label
        
        # åˆ›å»ºGraKeLå›¾å¯¹è±¡ï¼Œç¡®ä¿æä¾›æ­£ç¡®çš„æ ¼å¼
        try:
            # è½¬æ¢ä¸ºè¾¹åˆ—è¡¨æ ¼å¼
            edge_list1 = [(u, v) for u, v in G1.edges()]
            edge_list2 = [(u, v) for u, v in G2.edges()]
            
            # åˆ›å»ºå›¾å¯¹è±¡ï¼ŒåŒ…å«èŠ‚ç‚¹å’Œè¾¹æ ‡ç­¾
            g1 = Graph(edge_list1, node_labels=node_labels1, edge_labels=edge_labels1)
            g2 = Graph(edge_list2, node_labels=node_labels2, edge_labels=edge_labels2)
            
            # è®¡ç®—ç›¸ä¼¼æ€§
            similarity_matrix = wl_kernel.fit_transform([g1, g2])
            similarity = similarity_matrix[0, 1]
            
            return max(0.0, min(1.0, similarity))  # ç¡®ä¿åœ¨[0,1]èŒƒå›´å†…
            
        except Exception as kernel_error:
            print(f"Warning: GraKeL kernel calculation failed - {str(kernel_error)}. Using edge histogram fallback.")
            
            # ä½¿ç”¨è¾¹ç›´æ–¹å›¾ä½œä¸ºåå¤‡æ–¹æ¡ˆ
            try:
                edge_kernel = EdgeHistogram(normalize=True)
                edge_similarity = edge_kernel.fit_transform([g1, g2])[0, 1]
                return max(0.0, min(1.0, edge_similarity))
            except:
                # å¦‚æœæ‰€æœ‰å›¾æ ¸éƒ½å¤±è´¥ï¼Œä½¿ç”¨åŸºæœ¬ç›¸ä¼¼æ€§
                print(f"Warning: All graph kernels failed. Using basic structural similarity.")
                return compute_motif_similarity_3(motif1, motif2)
    
    except Exception as e:
        print(f"Warning: Graph kernel error - {str(e)}. Falling back to basic similarity.")
        # å›é€€åˆ°ç®€å•çš„ç›¸ä¼¼æ€§åº¦é‡
        return compute_motif_similarity_3(motif1, motif2)  # ä½¿ç”¨è½»é‡çº§å¯å‘å¼ä½œä¸ºå›é€€

# Choose the similarity function here:
similarity_function = enhanced_wl_kernel_similarity

# -------------------------
# Optimal Matching using Hungarian Algorithm
# -------------------------
def optimal_matching(motifs_A, motifs_B):
    n = len(motifs_A)
    m = len(motifs_B)
    sim_matrix = np.zeros((n, m))
    weight_matrix = np.zeros((n, m))
    
    for i in range(n):
        for j in range(m):
            sim = similarity_function(motifs_A[i], motifs_B[j])
            sim_matrix[i, j] = sim
            w_i = compute_motif_weight(motifs_A[i])
            w_j = compute_motif_weight(motifs_B[j])
            weight_matrix[i, j] = (w_i + w_j) / 2.0
            
    cost_matrix = 1 - sim_matrix
    row_ind, col_ind = linear_sum_assignment(cost_matrix)
    total_weighted_similarity = 0.0
    total_matched_weight = 0.0
    for i, j in zip(row_ind, col_ind):
        total_weighted_similarity += weight_matrix[i, j] * sim_matrix[i, j]
        total_matched_weight += weight_matrix[i, j]
    return total_weighted_similarity, total_matched_weight

# -------------------------
# Overall RNA Similarity Aggregation
# -------------------------
def compute_RNA_similarity(RNA1_motifs, RNA2_motifs):
    if not RNA1_motifs or not RNA2_motifs:
        return 0.0
    total_weight_RNA1 = sum(compute_motif_weight(m) for m in RNA1_motifs)
    total_weight_RNA2 = sum(compute_motif_weight(m) for m in RNA2_motifs)
    matched_sim, matched_weight = optimal_matching(RNA1_motifs, RNA2_motifs)
    S_ij = (2 * matched_sim) / (total_weight_RNA1 + total_weight_RNA2)
    return S_ij

# -------------------------
# Compute Similarity Map (Serial Version)
# -------------------------
def compute_similarity_map(RNA_motifs):
    RNA_ids = list(RNA_motifs.keys())
    similarity_map = {}
    pairs = [(i, j) for i in range(len(RNA_ids)) for j in range(i+1, len(RNA_ids))]
    total_pairs = len(pairs)
    processed = 0
    start_time = time.time()
    
    for pair in pairs:
        i, j = pair
        rna1 = RNA_ids[i]
        rna2 = RNA_ids[j]
        sim = compute_RNA_similarity(RNA_motifs[rna1], RNA_motifs[rna2])
        if sim > 0:  # Only save non-zero similarities
            similarity_map[(rna1, rna2)] = sim
        
        processed += 1
        if processed % 100 == 0:
            elapsed = time.time() - start_time
            print(f"Processed {processed}/{total_pairs} pairs... Elapsed time: {elapsed:.2f}s, Current similarity: {sim:.2f}")
    
    return similarity_map

# -------------------------
# Save Similarity Map to File
# -------------------------
def save_similarity_map(similarity_map, filename="similarity_results_structure_attr.txt"):
    with open(filename, "w") as f:
        for (rna1, rna2), sim in similarity_map.items():
            f.write(f"Similarity between {rna1} and {rna2}: {sim:.2f}\n")
    print(f"Full similarity map with non-zero scores saved to {filename}")

# -------------------------
# å¹¶è¡Œåˆ†å—å¤„ç†æµ‹è¯•ç‰ˆæœ¬
# -------------------------
import multiprocessing as mp
import os
import shutil
import math
from functools import partial

def create_test_database(source_db, test_db, sample_rnas=100):
    """ä»åŸæ•°æ®åº“æŠ½å–éƒ¨åˆ†æ•°æ®åˆ›å»ºæµ‹è¯•æ•°æ®åº“"""
    print(f"åˆ›å»ºæµ‹è¯•æ•°æ®åº“ï¼šä»{sample_rnas}ä¸ªRNAä¸­æŠ½å–æ•°æ®...")
    
    # è¿æ¥åŸæ•°æ®åº“
    source_conn = sqlite3.connect(source_db)
    source_cursor = source_conn.cursor()
    
    # è·å–æŒ‡å®šæ•°é‡çš„RNAåˆ—è¡¨
    source_cursor.execute("SELECT DISTINCT pdbid FROM data ORDER BY RANDOM() LIMIT ?", (sample_rnas,))
    test_rnas = [row[0] for row in source_cursor.fetchall()]
    
    print(f"é€‰æ‹©çš„RNAæ•°é‡: {len(test_rnas)}")
    
    # è·å–åŸè¡¨çš„åˆ—ä¿¡æ¯
    source_cursor.execute("PRAGMA table_info(data)")
    columns_info = source_cursor.fetchall()
    print(f"åŸè¡¨ç»“æ„ï¼š{len(columns_info)}åˆ—")
    for col in columns_info:
        print(f"  åˆ—{col[0]}: {col[1]} ({col[2]})")
    
    # è·å–è¿™äº›RNAçš„æ‰€æœ‰motifæ•°æ®ï¼Œæ˜ç¡®æŒ‡å®šåˆ—å
    column_names = [col[1] for col in columns_info]  # è·å–åˆ—å
    columns_str = ", ".join(column_names)
    
    placeholders = ','.join(['?' for _ in test_rnas])
    query = f"SELECT {columns_str} FROM data WHERE pdbid IN ({placeholders})"
    print(f"æŸ¥è¯¢SQL: {query}")
    
    source_cursor.execute(query, test_rnas)
    all_rows = source_cursor.fetchall()
    
    print(f"æå–çš„motifæ•°é‡: {len(all_rows)}")
    if all_rows:
        print(f"ç¬¬ä¸€è¡Œæ•°æ®é•¿åº¦: {len(all_rows[0])}")
        print(f"ç¬¬ä¸€è¡Œæ•°æ®ç±»å‹: {[type(x).__name__ for x in all_rows[0]]}")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®åº“
    test_conn = sqlite3.connect(test_db)
    test_cursor = test_conn.cursor()
    
    # æ‰‹åŠ¨åˆ›å»ºè¡¨ç»“æ„
    create_sql = "CREATE TABLE data ("
    column_definitions = []
    for col_info in columns_info:
        col_name = col_info[1]  # åˆ—å
        col_type = col_info[2]  # æ•°æ®ç±»å‹
        column_definitions.append(f"{col_name} {col_type}")
    create_sql += ", ".join(column_definitions) + ")"
    
    print(f"åˆ›å»ºè¡¨SQL: {create_sql}")
    test_cursor.execute(create_sql)
    
    # æ’å…¥æ•°æ®ï¼Œä½¿ç”¨æ­£ç¡®çš„åˆ—æ•°
    placeholders_insert = ','.join(['?' for _ in column_names])
    insert_sql = f"INSERT INTO data ({columns_str}) VALUES ({placeholders_insert})"
    
    print(f"æ’å…¥SQL: {insert_sql}")
    print(f"æœŸæœ›åˆ—æ•°: {len(column_names)}, å®é™…æ•°æ®åˆ—æ•°: {len(all_rows[0]) if all_rows else 0}")
    
    # éªŒè¯æ•°æ®å®Œæ•´æ€§
    if all_rows and len(all_rows[0]) != len(column_names):
        print(f"âŒ æ•°æ®åˆ—æ•°ä¸åŒ¹é…ï¼æœŸæœ›{len(column_names)}åˆ—ï¼Œå®é™…{len(all_rows[0])}åˆ—")
        # å°è¯•é‡æ–°æŸ¥è¯¢ï¼Œä½¿ç”¨*é€‰æ‹©æ‰€æœ‰åˆ—
        source_cursor.execute(f"SELECT * FROM data WHERE pdbid IN ({placeholders}) LIMIT 1", test_rnas)
        sample_row = source_cursor.fetchone()
        print(f"ä½¿ç”¨SELECT *çš„æ ·æœ¬è¡Œé•¿åº¦: {len(sample_row) if sample_row else 0}")
        
        # é‡æ–°è·å–å®Œæ•´æ•°æ®
        source_cursor.execute(f"SELECT * FROM data WHERE pdbid IN ({placeholders})", test_rnas)
        all_rows = source_cursor.fetchall()
        print(f"é‡æ–°è·å–çš„æ•°æ®è¡Œé•¿åº¦: {len(all_rows[0]) if all_rows else 0}")
    
    # æ’å…¥æ•°æ®
    test_cursor.executemany(insert_sql, all_rows)
    
    test_conn.commit()
    source_conn.close()
    test_conn.close()
    
    print(f"æµ‹è¯•æ•°æ®åº“åˆ›å»ºå®Œæˆï¼š{test_db}")
    print(f"åŒ…å« {len(test_rnas)} ä¸ªRNAï¼Œ{len(all_rows)} ä¸ªmotif")
    return test_rnas

def load_lightweight_index(db_path):
    """è½»é‡çº§åŠ è½½ï¼šåªè¯»å–RNA-motifç´¢å¼•ä¿¡æ¯"""
    print("æ„å»ºè½»é‡çº§ç´¢å¼•...")
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # è¯»å–pdbidï¼Œä¿æŒåŸå§‹å¤§å°å†™
    cursor.execute("SELECT DISTINCT pdbid FROM data")
    rna_list = [row[0] for row in cursor.fetchall()]  # ç§»é™¤.lower()
    
    conn.close()
    
    print(f"ç´¢å¼•æ„å»ºå®Œæˆï¼š{len(rna_list)} ä¸ªRNA")
    return rna_list

def create_rna_blocks(rna_list, block_size):
    """å°†RNAåˆ—è¡¨åˆ†å—"""
    blocks = [rna_list[i:i+block_size] for i in range(0, len(rna_list), block_size)]
    print(f"åˆ†å—å®Œæˆï¼š{len(rna_list)} ä¸ªRNAåˆ†æˆ {len(blocks)} å—ï¼Œæ¯å—çº¦ {block_size} ä¸ª")
    return blocks

def load_block_motifs(db_path, target_rna_list):
    """æŒ‰éœ€åŠ è½½ï¼šåªåŠ è½½æŒ‡å®šRNAçš„motifæ•°æ®"""
    if not target_rna_list:
        return []
    
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    
    # ä½¿ç”¨INæŸ¥è¯¢åªè·å–éœ€è¦çš„RNAçš„motif
    placeholders = ','.join(['?' for _ in target_rna_list])
    query = f"SELECT * FROM data WHERE pdbid IN ({placeholders})"
    cursor.execute(query, target_rna_list)
    
    rows = cursor.fetchall()
    columns = [desc[0] for desc in cursor.description]
    data = [dict(zip(columns, row)) for row in rows]
    
    print(f"åŠ è½½äº† {len(data)} ä¸ªmotifï¼Œæ¥è‡ª {len(target_rna_list)} ä¸ªRNA")
    
    conn.close()
    return data

def compute_block_pairs_similarity(block1_rnas, block2_rnas, all_motif_data):
    """è®¡ç®—ä¸¤ä¸ªå—ä¹‹é—´çš„RNAç›¸ä¼¼æ€§"""
    # ä½¿ç”¨ç°æœ‰å‡½æ•°è¿›è¡Œåˆ†ç»„
    RNA_motifs = group_motifs_by_RNA(all_motif_data)
    
    results = {}
    total_pairs = 0
    non_zero_pairs = 0
    
    for rna1 in block1_rnas:
        for rna2 in block2_rnas:
            # é¿å…é‡å¤è®¡ç®—ï¼ˆåªåœ¨å—å†…æ—¶éœ€è¦ï¼‰
            if block1_rnas == block2_rnas and rna1 >= rna2:
                continue
            
            total_pairs += 1
            
            # ç»Ÿä¸€ä½¿ç”¨å°å†™è¿›è¡ŒåŒ¹é…ï¼ˆä¸group_motifs_by_RNAä¿æŒä¸€è‡´ï¼‰
            rna1_key = rna1.lower()
            rna2_key = rna2.lower()
            
            # æ£€æŸ¥RNAæ˜¯å¦å­˜åœ¨äºæ•°æ®ä¸­
            if rna1_key not in RNA_motifs or rna2_key not in RNA_motifs:
                print(f"  è·³è¿‡ï¼š{rna1}({rna1_key}) æˆ– {rna2}({rna2_key}) ä¸åœ¨æ•°æ®ä¸­")
                continue
                
            # ç›´æ¥è°ƒç”¨ç°æœ‰å‡½æ•°ï¼Œé›¶ä¿®æ”¹
            sim = compute_RNA_similarity(RNA_motifs[rna1_key], RNA_motifs[rna2_key])
            
            if sim > 0:
                results[(rna1, rna2)] = sim
                non_zero_pairs += 1
                print(f"  ç›¸ä¼¼æ€§ï¼š{rna1} vs {rna2} = {sim:.3f}")
            else:
                print(f"  é›¶ç›¸ä¼¼æ€§ï¼š{rna1} vs {rna2} (motifæ•°: {len(RNA_motifs[rna1_key])}, {len(RNA_motifs[rna2_key])})")
    
    print(f"  å—å¯¹ç»Ÿè®¡ï¼šæ€»è®¡{total_pairs}å¯¹ï¼Œéé›¶{non_zero_pairs}å¯¹ï¼Œä¿å­˜{len(results)}å¯¹")
    return results

def distribute_block_pairs(total_blocks, num_processes):
    """å°†å—å¯¹åˆ†é…ç»™å¤šä¸ªè¿›ç¨‹"""
    # ç”Ÿæˆæ‰€æœ‰å—å¯¹ç»„åˆ
    all_pairs = []
    for i in range(total_blocks):
        for j in range(i, total_blocks):
            all_pairs.append((i, j))
    
    # å¹³å‡åˆ†é…ç»™è¿›ç¨‹
    pairs_per_process = len(all_pairs) // num_processes
    process_tasks = []
    
    for p in range(num_processes):
        start_idx = p * pairs_per_process
        if p == num_processes - 1:  # æœ€åä¸€ä¸ªè¿›ç¨‹å¤„ç†å‰©ä½™çš„
            end_idx = len(all_pairs)
        else:
            end_idx = (p + 1) * pairs_per_process
        
        process_tasks.append(all_pairs[start_idx:end_idx])
    
    return process_tasks

def process_worker_with_file_output(assigned_pairs, db_path, rna_blocks, output_file, worker_id):
    """Workerè¿›ç¨‹ï¼šè®¡ç®—å¹¶ç›´æ¥å†™æ–‡ä»¶"""
    print(f"Worker {worker_id}: å¼€å§‹å¤„ç† {len(assigned_pairs)} ä¸ªå—å¯¹...")
    
    processed_pairs = 0
    total_similarities = 0
    
    with open(output_file, "w") as f:
        for pair_idx, (block_i, block_j) in enumerate(assigned_pairs):
            print(f"Worker {worker_id}: å¤„ç†å—å¯¹ ({block_i},{block_j}) - {pair_idx+1}/{len(assigned_pairs)}")
            
            # ç¡®å®šéœ€è¦åŠ è½½çš„RNA
            target_rnas = list(set(rna_blocks[block_i] + rna_blocks[block_j]))
            
            # åŠ è½½å½“å‰å—å¯¹çš„æ•°æ®
            block_data = load_block_motifs(db_path, target_rnas)
            
            if not block_data:
                print(f"Worker {worker_id}: å—å¯¹ ({block_i},{block_j}) æ— æ•°æ®ï¼Œè·³è¿‡")
                continue
            
            # è®¡ç®—ç›¸ä¼¼æ€§
            results = compute_block_pairs_similarity(rna_blocks[block_i], rna_blocks[block_j], block_data)
            
            # ç›´æ¥å†™å…¥æ–‡ä»¶
            for (rna1, rna2), sim in results.items():
                f.write(f"Similarity between {rna1} and {rna2}: {sim:.2f}\n")
            
            total_similarities += len(results)
            processed_pairs += 1
            
            # å®æ—¶flushï¼Œç¡®ä¿æ•°æ®å†™å…¥
            f.flush()
            
            # é‡Šæ”¾å†…å­˜
            del block_data
    
    print(f"Worker {worker_id}: å®Œæˆï¼å¤„ç†äº† {processed_pairs} ä¸ªå—å¯¹ï¼Œè®¡ç®—äº† {total_similarities} ä¸ªç›¸ä¼¼æ€§")
    return total_similarities

def merge_result_files(temp_files, final_output):
    """åˆå¹¶æ‰€æœ‰workerçš„ç»“æœæ–‡ä»¶"""
    print("åˆå¹¶ç»“æœæ–‡ä»¶...")
    total_lines = 0
    
    with open(final_output, "w") as outf:
        for temp_file in temp_files:
            if os.path.exists(temp_file):
                with open(temp_file, "r") as inf:
                    lines = inf.readlines()
                    outf.writelines(lines)
                    total_lines += len(lines)
                os.remove(temp_file)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
    
    print(f"åˆå¹¶å®Œæˆï¼š{len(temp_files)} ä¸ªæ–‡ä»¶ï¼Œæ€»è®¡ {total_lines} è¡Œç»“æœ")
    return total_lines

def parallel_blockwise_similarity_map(db_path, block_size=10, num_processes=2, output_file="test_results.txt"):
    """å¹¶è¡Œåˆ†å—è®¡ç®—ç›¸ä¼¼æ€§å›¾è°±ï¼ˆç­–ç•¥1ï¼šåˆ†åˆ«å†™æ–‡ä»¶ï¼‰"""
    print("ğŸš€ å¯åŠ¨å¹¶è¡Œåˆ†å—ç›¸ä¼¼æ€§è®¡ç®—...")
    start_time = time.time()
    
    # ç¬¬1æ­¥ï¼šè½»é‡çº§ç´¢å¼•
    rna_list = load_lightweight_index(db_path)
    rna_blocks = create_rna_blocks(rna_list, block_size)
    
    total_blocks = len(rna_blocks)
    total_pairs = total_blocks * (total_blocks + 1) // 2
    print(f"é…ç½®ï¼š{len(rna_list)} ä¸ªRNAï¼Œ{total_blocks} å—ï¼Œ{num_processes} è¿›ç¨‹")
    print(f"é¢„è®¡è®¡ç®— {total_pairs} ä¸ªå—å¯¹")
    
    # ç¬¬2æ­¥ï¼šä»»åŠ¡åˆ†é…
    process_tasks = distribute_block_pairs(total_blocks, num_processes)
    
    for i, task in enumerate(process_tasks):
        print(f"è¿›ç¨‹ {i}: åˆ†é…åˆ° {len(task)} ä¸ªå—å¯¹")
    
    # ç¬¬3æ­¥ï¼šåˆ›å»ºä¸´æ—¶ç›®å½•
    temp_dir = "temp_results"
    os.makedirs(temp_dir, exist_ok=True)
    
    # ç¬¬4æ­¥ï¼šå¹¶è¡Œè®¡ç®—
    print("å¯åŠ¨å¹¶è¡Œè®¡ç®—...")
    
    try:
        with mp.Pool(processes=num_processes) as pool:
            async_results = []
            temp_files = []
            
            for i, assigned_pairs in enumerate(process_tasks):
                if not assigned_pairs:  # è·³è¿‡ç©ºä»»åŠ¡
                    continue
                    
                # æ¯ä¸ªworkeræœ‰è‡ªå·±çš„è¾“å‡ºæ–‡ä»¶
                worker_output = f"{temp_dir}/worker_{i}_results.txt"
                temp_files.append(worker_output)
                
                result = pool.apply_async(
                    process_worker_with_file_output,
                    (assigned_pairs, db_path, rna_blocks, worker_output, i)
                )
                async_results.append(result)
            
            # ç­‰å¾…æ‰€æœ‰workerå®Œæˆ
            total_similarities = 0
            for async_result in async_results:
                worker_similarities = async_result.get()  # ç­‰å¾…å®Œæˆ
                total_similarities += worker_similarities
        
        # ç¬¬5æ­¥ï¼šåˆå¹¶ç»“æœæ–‡ä»¶
        final_lines = merge_result_files(temp_files, output_file)
        
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        
        # ç»Ÿè®¡ç»“æœ
        elapsed = time.time() - start_time
        print(f"âœ… å¹¶è¡Œè®¡ç®—å®Œæˆï¼")
        print(f"â±ï¸  æ€»è€—æ—¶ï¼š{elapsed:.1f}ç§’")
        print(f"ğŸ“ˆ è®¡ç®—äº† {total_similarities} ä¸ªç›¸ä¼¼æ€§å¯¹")
        print(f"ğŸ“ ç»“æœä¿å­˜åˆ°ï¼š{output_file}")
        
        return total_similarities
        
    except Exception as e:
        print(f"âŒ å¹¶è¡Œè®¡ç®—å‡ºé”™ï¼š{e}")
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if os.path.exists(temp_dir):
            shutil.rmtree(temp_dir)
        raise

def quick_test_main():
    """å¿«é€Ÿæµ‹è¯•ä¸»ç¨‹åºï¼š100ä¸ªRNAï¼Œ2ä¸ªworkerï¼Œå‡ åˆ†é’Ÿå®Œæˆ"""
    print("ğŸ§ª å¿«é€Ÿæµ‹è¯•ç¨‹åºå¯åŠ¨...")
    print("é…ç½®ï¼š100ä¸ªRNAï¼Œ10å—ï¼Œ2ä¸ªworkerï¼Œé¢„æœŸ2-5åˆ†é’Ÿå®Œæˆ")
    
    start_time = time.time()
    
    # é…ç½®å‚æ•° - ä¿®å¤ï¼šä½¿ç”¨å®Œæ•´çš„ATLASæ•°æ®åº“
    source_db = "../../../data/ATLAS.db"  # ä½¿ç”¨å®Œæ•´çš„æ­£å¸¸æ•°æ®åº“
    test_db = "../../../data/ATLAS2025_QUICK_TEST_FIXED.db"  # åˆ›å»ºæ–°çš„æµ‹è¯•æ•°æ®åº“
    output_file = "../../../postprocessing/results2025/similarity_map/quick_test_results.txt"
    
    try:
        # ç¬¬1æ­¥ï¼šåˆ›å»ºå°æµ‹è¯•æ•°æ®åº“ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        print("ğŸ“Š ç¬¬1æ­¥ï¼šæ£€æŸ¥æµ‹è¯•æ•°æ®åº“...")
        if os.path.exists(test_db):
            print(f"æµ‹è¯•æ•°æ®åº“å·²å­˜åœ¨ï¼š{test_db}")
            # è¯»å–ç°æœ‰æ•°æ®åº“çš„RNAæ•°é‡
            conn = sqlite3.connect(test_db)
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(DISTINCT pdbid) FROM data")
            rna_count = cursor.fetchone()[0]
            conn.close()
            print(f"ç°æœ‰æ•°æ®åº“åŒ…å« {rna_count} ä¸ªRNA")
        else:
            test_rnas = create_test_database(source_db, test_db, sample_rnas=100)
        
        # ç¬¬2æ­¥ï¼šå¹¶è¡Œè®¡ç®—
        print("âš¡ ç¬¬2æ­¥ï¼šå¯åŠ¨å¹¶è¡Œè®¡ç®—...")
        total_similarities = parallel_blockwise_similarity_map(
            db_path=test_db,
            block_size=10,      # æ¯å—10ä¸ªRNAï¼ˆå…±çº¦10å—ï¼‰
            num_processes=2,    # 2ä¸ªworker
            output_file=output_file
        )
        
        # ç¬¬3æ­¥ï¼šéªŒè¯ç»“æœ
        print("ğŸ” ç¬¬3æ­¥ï¼šéªŒè¯ç»“æœ...")
        if os.path.exists(output_file):
            with open(output_file, "r") as f:
                lines = f.readlines()
            print(f"ç»“æœæ–‡ä»¶è¡Œæ•°ï¼š{len(lines)}")
            
            # æ˜¾ç¤ºå‰å‡ è¡Œä½œä¸ºæ ·æœ¬
            print("ç»“æœæ ·æœ¬ï¼š")
            for i, line in enumerate(lines[:5]):
                print(f"  {line.strip()}")
            if len(lines) > 5:
                print(f"  ... (è¿˜æœ‰ {len(lines)-5} è¡Œ)")
        
        # ç»Ÿè®¡æ€»ç»“
        elapsed = time.time() - start_time
        print(f"\nâœ… å¿«é€Ÿæµ‹è¯•å®Œæˆï¼")
        print(f"â±ï¸  æ€»è€—æ—¶ï¼š{elapsed:.1f}ç§’")
        print(f"ğŸ“ˆ è®¡ç®—ç›¸ä¼¼æ€§å¯¹ï¼š{total_similarities}")
        print(f"ğŸ“ ç»“æœæ–‡ä»¶ï¼š{output_file}")
        
        return total_similarities
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥ï¼š{e}")
        import traceback
        traceback.print_exc()
        return None

# -------------------------
# Main Pipeline
# -------------------------
def main():
    print("Starting RNA similarity analysis (serial version)...")
    db_path = "../../../data/ATLAS.db"  # ä¿®å¤ï¼šä½¿ç”¨å®Œæ•´çš„ATLASæ•°æ®åº“
    data = load_data(db_path)
    RNA_motifs = group_motifs_by_RNA(data)
    print(f"Total RNAs: {len(RNA_motifs)}")
    
    print("Calculating full RNA similarity map (serial version)...")
    similarity_map = compute_similarity_map(RNA_motifs)
    save_similarity_map(similarity_map, filename="../../../postprocessing/results2025/similarity_map/similarity_results_ATLAS2025.txt")

if __name__ == "__main__":
    # è¿è¡Œå¿«é€Ÿæµ‹è¯•
    quick_test_main()
